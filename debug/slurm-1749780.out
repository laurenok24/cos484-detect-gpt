Saving results to absolute path: /scratch/network/tb19/cos484-detect-gpt/tmp_results/main/gpt2-xl-t5-3b-temp/2023-04-19-12-52-23-634461-fp32-0.3-1-xsum-500
Using cache dir /scratch/network/tb19/.cache
Loading BASE model gpt2-xl...
Loading mask filling model t5-3b...
MOVING BASE MODEL TO GPU...Using the latest cached version of the module from /home/tb19/.cache/huggingface/modules/datasets_modules/datasets/xsum/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71 (last modified on Tue Apr 18 20:13:01 2023) since it couldn't be found locally at xsum., or remotely on the Hugging Face Hub.
Found cached dataset xsum (/scratch/network/tb19/.cache/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)
Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors
DONE (0.96s)
Loading dataset xsum...
Total number of samples: 1829
Average number of words: 306.9710224166211
Generating samples for batch 0 of 10
Generating samples for batch 1 of 10
Generating samples for batch 2 of 10
Generating samples for batch 3 of 10
Generating samples for batch 4 of 10
Generating samples for batch 5 of 10
Generating samples for batch 6 of 10
Generating samples for batch 7 of 10
Generating samples for batch 8 of 10
Generating samples for batch 9 of 10
Writing raw data to tmp_results/main/gpt2-xl-t5-3b-temp/2023-04-19-12-52-23-634461-fp32-0.3-1-xsum-500/raw_data.json
Computing likelihood criterion:   0%|          | 0/10 [00:00<?, ?it/s]Computing likelihood criterion:  10%|█         | 1/10 [00:06<00:54,  6.08s/it]Computing likelihood criterion:  20%|██        | 2/10 [00:12<00:48,  6.09s/it]Computing likelihood criterion:  30%|███       | 3/10 [00:18<00:42,  6.09s/it]Computing likelihood criterion:  40%|████      | 4/10 [00:24<00:36,  6.06s/it]Computing likelihood criterion:  50%|█████     | 5/10 [00:30<00:30,  6.08s/it]Computing likelihood criterion:  60%|██████    | 6/10 [00:36<00:24,  6.09s/it]Computing likelihood criterion:  70%|███████   | 7/10 [00:42<00:18,  6.08s/it]Computing likelihood criterion:  80%|████████  | 8/10 [00:48<00:12,  6.08s/it]Computing likelihood criterion:  90%|█████████ | 9/10 [00:54<00:06,  6.09s/it]Computing likelihood criterion: 100%|██████████| 10/10 [01:00<00:00,  6.09s/it]Computing likelihood criterion: 100%|██████████| 10/10 [01:00<00:00,  6.08s/it]
likelihood_threshold ROC AUC: 0.8791599999999999, PR AUC: 0.8496150223780717
Computing rank criterion:   0%|          | 0/10 [00:00<?, ?it/s]Computing rank criterion:  10%|█         | 1/10 [00:06<00:56,  6.29s/it]Computing rank criterion:  20%|██        | 2/10 [00:12<00:50,  6.30s/it]Computing rank criterion:  30%|███       | 3/10 [00:18<00:44,  6.29s/it]Computing rank criterion:  40%|████      | 4/10 [00:25<00:37,  6.27s/it]Computing rank criterion:  50%|█████     | 5/10 [00:31<00:31,  6.28s/it]Computing rank criterion:  60%|██████    | 6/10 [00:37<00:25,  6.29s/it]Computing rank criterion:  70%|███████   | 7/10 [00:43<00:18,  6.28s/it]Computing rank criterion:  80%|████████  | 8/10 [00:50<00:12,  6.28s/it]Computing rank criterion:  90%|█████████ | 9/10 [00:56<00:06,  6.28s/it]Computing rank criterion: 100%|██████████| 10/10 [01:02<00:00,  6.28s/it]Computing rank criterion: 100%|██████████| 10/10 [01:02<00:00,  6.28s/it]
rank_threshold ROC AUC: 0.790524, PR AUC: 0.8091126933929396
Computing log_rank criterion:   0%|          | 0/10 [00:00<?, ?it/s]Computing log_rank criterion:  10%|█         | 1/10 [00:06<00:56,  6.30s/it]Computing log_rank criterion:  20%|██        | 2/10 [00:12<00:50,  6.30s/it]Computing log_rank criterion:  30%|███       | 3/10 [00:18<00:44,  6.29s/it]Computing log_rank criterion:  40%|████      | 4/10 [00:25<00:37,  6.27s/it]Computing log_rank criterion:  50%|█████     | 5/10 [00:31<00:31,  6.29s/it]Computing log_rank criterion:  60%|██████    | 6/10 [00:37<00:25,  6.29s/it]Computing log_rank criterion:  70%|███████   | 7/10 [00:43<00:18,  6.28s/it]Computing log_rank criterion:  80%|████████  | 8/10 [00:50<00:12,  6.28s/it]Computing log_rank criterion:  90%|█████████ | 9/10 [00:56<00:06,  6.28s/it]Computing log_rank criterion: 100%|██████████| 10/10 [01:02<00:00,  6.28s/it]Computing log_rank criterion: 100%|██████████| 10/10 [01:02<00:00,  6.28s/it]
log_rank_threshold ROC AUC: 0.908568, PR AUC: 0.887564520929254
Computing entropy criterion:   0%|          | 0/10 [00:00<?, ?it/s]Computing entropy criterion:  10%|█         | 1/10 [00:06<00:55,  6.15s/it]Computing entropy criterion:  20%|██        | 2/10 [00:12<00:49,  6.15s/it]Computing entropy criterion:  30%|███       | 3/10 [00:18<00:43,  6.15s/it]Computing entropy criterion:  40%|████      | 4/10 [00:24<00:36,  6.12s/it]Computing entropy criterion:  50%|█████     | 5/10 [00:30<00:30,  6.14s/it]Computing entropy criterion:  60%|██████    | 6/10 [00:36<00:24,  6.14s/it]Computing entropy criterion:  70%|███████   | 7/10 [00:42<00:18,  6.13s/it]Computing entropy criterion:  80%|████████  | 8/10 [00:49<00:12,  6.13s/it]Computing entropy criterion:  90%|█████████ | 9/10 [00:55<00:06,  6.13s/it]Computing entropy criterion: 100%|██████████| 10/10 [01:01<00:00,  6.13s/it]Computing entropy criterion: 100%|██████████| 10/10 [01:01<00:00,  6.14s/it]
Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
entropy_threshold ROC AUC: 0.5641800000000001, PR AUC: 0.5568465027086535
Beginning supervised evaluation with roberta-base-openai-detector...
Evaluating real:   0%|          | 0/10 [00:00<?, ?it/s]Evaluating real:  10%|█         | 1/10 [00:00<00:02,  3.77it/s]Evaluating real:  20%|██        | 2/10 [00:00<00:01,  5.06it/s]Evaluating real:  30%|███       | 3/10 [00:00<00:01,  5.70it/s]Evaluating real:  40%|████      | 4/10 [00:00<00:01,  5.92it/s]Evaluating real:  50%|█████     | 5/10 [00:00<00:00,  6.16it/s]Evaluating real:  60%|██████    | 6/10 [00:01<00:00,  6.32it/s]Evaluating real:  70%|███████   | 7/10 [00:01<00:00,  6.41it/s]Evaluating real:  80%|████████  | 8/10 [00:01<00:00,  6.48it/s]Evaluating real:  90%|█████████ | 9/10 [00:01<00:00,  6.53it/s]Evaluating real: 100%|██████████| 10/10 [00:01<00:00,  6.59it/s]Evaluating real: 100%|██████████| 10/10 [00:01<00:00,  6.15it/s]
Evaluating fake:   0%|          | 0/10 [00:00<?, ?it/s]Evaluating fake:  10%|█         | 1/10 [00:00<00:01,  7.32it/s]Evaluating fake:  20%|██        | 2/10 [00:00<00:01,  7.34it/s]Evaluating fake:  30%|███       | 3/10 [00:00<00:00,  7.34it/s]Evaluating fake:  40%|████      | 4/10 [00:00<00:00,  7.34it/s]Evaluating fake:  50%|█████     | 5/10 [00:00<00:00,  7.36it/s]Evaluating fake:  60%|██████    | 6/10 [00:00<00:00,  7.36it/s]Evaluating fake:  70%|███████   | 7/10 [00:00<00:00,  7.37it/s]Evaluating fake:  80%|████████  | 8/10 [00:01<00:00,  7.37it/s]Evaluating fake:  90%|█████████ | 9/10 [00:01<00:00,  7.38it/s]Evaluating fake: 100%|██████████| 10/10 [00:01<00:00,  7.37it/s]Evaluating fake: 100%|██████████| 10/10 [00:01<00:00,  7.36it/s]
roberta-base-openai-detector ROC AUC: 0.967694, PR AUC: 0.96897885472257
Beginning supervised evaluation with roberta-large-openai-detector...
Traceback (most recent call last):
  File "/scratch/network/tb19/cos484-detect-gpt/run.py", line 887, in <module>
    baseline_outputs.append(eval_supervised(data, model='roberta-large-openai-detector'))
  File "/scratch/network/tb19/cos484-detect-gpt/run.py", line 693, in eval_supervised
    detector = transformers.AutoModelForSequenceClassification.from_pretrained(model, cache_dir=cache_dir).to(DEVICE)
  File "/home/tb19/.conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 434, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/tb19/.conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 776, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/tb19/.conda/envs/pytorch/lib/python3.10/site-packages/transformers/configuration_utils.py", line 559, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/tb19/.conda/envs/pytorch/lib/python3.10/site-packages/transformers/configuration_utils.py", line 614, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/tb19/.conda/envs/pytorch/lib/python3.10/site-packages/transformers/utils/hub.py", line 409, in cached_file
    resolved_file = hf_hub_download(
  File "/home/tb19/.conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1136, in hf_hub_download
    with open(ref_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/network/tb19/.cache/models--roberta-large-openai-detector/refs/main'
